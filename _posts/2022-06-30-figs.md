---
layout:             post
title:              "FIGS: Attaining XGBoost-level performance with the interpretability and speed of CART"
date:               2022-06-30  9:00:00
author:             <a href="https://csinva.io/">Chandan Singh</a> and <a href="https://sites.google.com/view/yanshuotan/home">Yan Shuo Tan</a> and <a href="https://binyu.stat.berkeley.edu/">Bin Yu</a>
img:                /assets/figs/figs_intro.gif
excerpt_separator:  <!--more-->
visible:            False
show_comments:      False
---

<!-- twitter -->
<meta name="twitter:title" content="FIGS: Attaining XGBoost-level performance =with the interpretability and speed of CART">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://bair.berkeley.edu/static/blog/figs/figs_intro.gif">

<meta name="keywords" content="figs,interpretability,trees,imodels">
<meta name="description" content="The BAIR Blog">
<meta name="author" content="Chandan Singh, Yan Shuo Tan, Bin Yu">

<p style="text-align:center;">
    <a href="https://arxiv.org/abs/2201.11931"><img src="https://bair.berkeley.edu/static/blog/figs/figs_intro.gif" width="90%"></a>
<br>
<b>FIGS (Fast Interpretable Greedy-tree Sums): </b><i>A method for building interpretable models by simultaneously growing an ensemble of decision trees in competition with one another.</i>
</p>

Recent machine-learning advances have led to increasingly complex predictive models, often at the cost of interpretability. We often need interpretability, particularly in high-stakes applications such as in clinical decision-making; interpretable models help with all kinds of things, such as identifying errors, leveraging domain knowledge, and making speedy predictions.

In this blog post we'll cover [FIGS](https://arxiv.org/abs/2201.11931), a new method for fitting an *interpretable model* that takes the form of a sum of trees. Real-world experiments and theoretical results show that FIGS can effectively adapt to a wide range of structure in data, achieving state-of-the-art performance in several settings, all without sacrificing interpretability.
<!--more-->

## How does FIGS work?

Intuitively, FIGS works by extending CART, a typical greedy algorithm for growing a decision tree, to consider growing a *sum* of trees *simultaneously* (see Fig 1). At each iteration, FIGS may grow any existing tree it has already started or start a new tree; it greedily selects whichever rule reduces the total unexplained variance (or an alternative splitting criterion) the most. To keep the trees in sync with one another, each tree is made to predict the *residuals* remaining after summing the predictions of all other trees (see [the paper](https://arxiv.org/abs/2201.11931) for more details).

FIGS is intuitively similar to ensemble approaches such as gradient boosting / random forest, but importantly since all trees are grown to compete with each other the model can adapt more to the underlying structure in the data. The number of trees and size/shape of each tree emerge automatically from the data rather than being manually specified.

<p style="text-align:center;">
    <a href="https://github.com/csinva/imodels"><img src="https://bair.berkeley.edu/static/blog/figs/figs_fitting.gif" width="90%"></a>
<br>
<b>Fig 1. </b><i>High-level intuition for how FIGS fits a model.</i>
</p>



## An example using `FIGS`

Using FIGS is extremely simple. It is easily installable through the [imodels package](https://github.com/csinva/imodels) (`pip install imodels`) and then can be used in the same way as standard scikit-learn models: simply import a classifier or regressor and use the `fit` and `predict` methods. Here's a full example of using it on a sample clinical dataset in which the target is risk of cervical spine injury (CSI).

```python
from imodels import FIGSClassifier, get_clean_dataset
from sklearn.model_selection import train_test_split

# prepare data (in this a sample clinical dataset)
X, y, feat_names = get_clean_dataset('csi_pecarn_pred')
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.33, random_state=42)

# fit the model
model = FIGSClassifier(max_rules=4)  # initialize a model
model.fit(X_train, y_train)   # fit model
preds = model.predict(X_test) # discrete predictions: shape is (n_test, 1)
preds_proba = model.predict_proba(X_test) # predicted probabilities: shape is (n_test, n_classes)

# visualize the model
model.plot(feature_names=feat_names, filename='out.svg', dpi=300)
```

This results in a simple model -- it contains only 4 splits (since we specified that the model should have no more than 4 splits (`max_rules=4`). Predictions are made by dropping a sample down every tree, and <i>summing</i> the risk adjustment values obtained from the resulting leaves of each tree. This model is extremely interpretable, as a physician can now (i) easily make predictions using the 4 relevant features and (ii) vet the model to ensure it matches their domain expertise. Note that this model is just for illustration purposes, and achieves ~84\% accuracy.


<p style="text-align:center;">
    <a href="https://github.com/csinva/imodels"><img src="https://bair.berkeley.edu/static/blog/figs/figs_csi_model_small.svg" width="85%"></a>
<br>
<i><b>Fig 2.</b> Simple model learned by FIGS for predicting risk of cervical spinal injury. </i>
</p>

If we want a more flexible model, we can also remove the constraint on the number of rules (changing the code to `model = FIGSClassifier()`), resulting in a larger model (see Fig 3). Note that the number of trees and how balanced they are emerges from the structure of the data -- only the total number of rules may be specified.

<p style="text-align:center;">
    <a href="https://github.com/csinva/imodels"><img src="https://bair.berkeley.edu/static/blog/figs/figs_csi_model_large.svg" width="100%"></a>
<br>
<i><b>Fig 3.</b> Slightly larger model learned by FIGS for predicting risk of cervical spinal injury. </i>
</p>


## How well does FIGS perform?

In many cases when interpretability is desired, such as [clinical-decision-rule modeling](https://arxiv.org/abs/2205.15135), FIGS is able to achieve state-of-the-art performance. For example, Fig 4 shows different datasets where FIGS achieves excellent performance, particularly when limited to using very few total splits.

<p style="text-align:center;">
    <a href="https://github.com/csinva/imodels"><img src="https://bair.berkeley.edu/static/blog/figs/figs_classification.png" width="100%"></a>
<br>
<i><b>Fig 4.</b> FIGS predicts well with very few splits. </i>
</p>


## Why does FIGS perform well?

FIGS is motivated by the observation that single decision trees often have splits that are repeated in different branches, which may occur when there is [additive structure](https://proceedings.mlr.press/v151/shuo-tan22a/shuo-tan22a.pdf) in the data. Having multiple trees helps to avoid this by disentangling the additive components into separate trees.

## Conclusion

Overall, interpretable modeling offers an alternative to common black-box modeling, and in many cases can offer massive improvements in terms of efficiency and transparency without suffering from a loss in performance.

<hr>

*This post is based on two papers: [FIGS](https://arxiv.org/abs/2201.11931) and [G-FIGS](https://arxiv.org/abs/2205.15135) -- all code is available through the [imodels package](https://github.com/csinva/imodels). This is joint work with [Keyan Nasseri](https://www.linkedin.com/in/nasseri/), [Abhineet Agarwal](https://www.linkedin.com/in/abhineet-agarwal-126171185/), [James Duncan](https://www.linkedin.com/in/james-pc-duncan/), [Omer Ronen](https://www.linkedin.com/in/omer-ronen-48ba9412a/?originalSubdomain=il), and [Aaron Kornblith](https://profiles.ucsf.edu/aaron.kornblith).*
