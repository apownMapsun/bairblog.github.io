---
layout:             post
title:              "Fully Autonomous Real-World Reinforcement Learning with Applications to Mobile Manipulation"
date:               2023-01-20  9:00:00
author:             <a href="https://jorbik.info/">Jędrzej Orbik</a>, <a href="https://charlesjsun.github.io/">Charles Sun</a>, <a href="https://cdevin.github.io/">Coline Devin</a>, <a href="https://www.fracturedplane.com/">Glen Berseth</a>
img:                assets/relmm/title.png)
excerpt_separator:  <!--more-->
visible:            True
show_comments:      False
---

<!-- twitter -->
<meta name="twitter:title" content="Fully Autonomous Real-World Reinforcement Learning with Applications to Mobile Manipulation">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="assets/relmm/title.png">

<meta name="keywords" content="mobile manipulation, reinforcement learning, reset-free">
<meta name="description" content="The BAIR Blog">
<meta name="author" content="Jędrzej Orbik, Charles Sun, Coline Devin, Glen Berseth">

Reinforcement learning provides a conceptual framework for autonomous agents to learn from experience, analogously to how one might train a pet with treats. But practical applications of reinforcement learning are often far from natural: instead of using RL to learn through trial and error by actually attempting the desired task, typical RL applications use a separate (usually simulated) training phase. For example, [AlphaGo][1] did not learn to play Go by competing against thousands of humans, but rather by playing against itself in simulation. While this kind of simulated training is appealing for games where the rules are perfectly known, applying this to real world domains such as robotics can require a range of complex approaches, such as [the use of simulated data][2], or instrumenting real-world environments in various ways to make training feasible [under laboratory conditions][3]. Can we instead devise reinforcement learning systems for robots that allow them to learn directly "on-the-job", while performing the task that they are required to do? In this blog post, we will discuss ReLMM, a system that we developed that learns to clean up a room directly with a real robot via continual learning.

<p style="text-align:center;">
<img src="https://bair.berkeley.edu/static/blog/relmm/image8.gif" width="48%">
<img src="https://bair.berkeley.edu/static/blog/relmm/image12.gif" width="48%">
<img src="https://bair.berkeley.edu/static/blog/relmm/image3.gif" width="48%">
<img src="https://bair.berkeley.edu/static/blog/relmm/image2.gif" width="48%">
<br>
<i>We evaluate our method on different tasks that range in difficulty. The top-left task has uniform white blobs to pickup with no obstacles, while other rooms have objects of diverse shapes and colors, obstacles that increase navigation difficulty and obscure the objects and patterned rugs that make it difficult to see the objects against the ground.</i>
</p>

<!--more-->

To enable “on-the-job” training in the real world, the difficulty of collecting more experience is prohibitive. If we can make training in the real world easier, by making the data gathering process more autonomous without requiring human monitoring or intervention, we can further benefit from the simplicity of agents that learn from experience. In this work, we design an “on-the-job” mobile robot training system for cleaning by learning to grasp objects throughout different rooms.

# Lesson 1: The Benefits of Modular Policies for Robots.

People are not born one day and performing job interviews the next. There are many levels of tasks people learn before they apply for a job as we start with the easier ones and build on them. In ReLMM, we make use of this concept by allowing robots to train common-reusable skills, such as grasping, by first encouraging the robot to prioritize training these skills before learning later skills, such as navigation. Learning in this fashion has two advantages for robotics. The first advantage is that when an agent focuses on learning a skill, it is more efficient at collecting data around the local state distribution for that skill. 

<p style="text-align:center">
<img src="https://bair.berkeley.edu/static/blog/relmm/image13.png"  width="50%">
<br>
</p>

That is shown in the figure above, where we evaluated the amount of prioritized grasping experience needed to result in efficient mobile manipulation training. The second advantage to a multi-level learning approach is that we can inspect the models trained for different tasks and ask them questions, such as, “can you grasp anything right now” which is helpful for navigation training that we describe next.

<p style="text-align:center;">
<img src="https://bair.berkeley.edu/static/blog/relmm/image14.png"  width="50%">
<br>
</p>

Training this multi-level policy was not only more efficient than learning both skills at the same time but it allowed for the grasping controller to inform the navigation policy. Having a model that estimates the uncertainty in its grasp success (**Ours** above) can be used to improve navigation exploration by skipping areas without graspable objects, in contrast to **No Uncertainty Bonus** which does not use this information. The model can also be used to relabel data during training so that in the unlucky case when the grasping model was unsuccessful trying to grasp an object within its reach, the grasping policy can still provide some signal by indicating that an object was there but the grasping policy has not yet learned how to grasp it. Moreover, learning modular models has engineering benefits. Modular training allows for reusing skills that are easier to learn and can enable building intelligent systems one piece at a time. This is beneficial for many reasons, including safety evaluation and understanding.

# Lesson 2: Learning systems beat hand-coded systems, given time

<p style="text-align:center">
<img src="https://bair.berkeley.edu/static/blog/relmm/image15.png" width="50%">
<br>
</p>

Many robotics tasks that we see today can be solved to varying levels of success using hand-engineered controllers. For our room cleaning task, we designed a hand-engineered controller that locates objects using image clustering and turns towards the nearest detected object at each step. This expertly designed controller performs very well on the visually salient balled socks and takes reasonable paths around the obstacles **but it can not learn an optimal path to collect the objects quickly, and it struggles with visually diverse rooms**. As shown in video 3 below, the scripted policy gets distracted by the white patterned carpet while trying to locate more white objects to grasp.

<p style="text-align:center;">
1) <img src="https://bair.berkeley.edu/static/blog/relmm/image5.gif" width="45%">
2) <img src="https://bair.berkeley.edu/static/blog/relmm/image6.gif" width="45%">
<br>
3) <img src="https://bair.berkeley.edu/static/blog/relmm/image1.gif" width="45%">
4) <img src="https://bair.berkeley.edu/static/blog/relmm/image9.png" width="45%">
<br>
<i>We show a comparison between (1) our policy at the beginning of training (2) our policy at the end of training (3) the scripted policy. In (4) we can see the robot's performance improve over time, and eventually exceed the scripted policy at quickly collecting the objects in the room.</i>
</p>

Given we can use experts to code this hand-engineered controller, what is the purpose of learning? An important limitation of hand-engineered controllers is that they are tuned for a particular task, for example, grasping white objects. When diverse objects are introduced, which differ in color and shape, the original tuning may no longer be optimal. Rather than requiring further hand-engineering, our learning-based method is able to adapt itself to various tasks by collecting its own experience.

However, the most important lesson is that even if the hand-engineered controller is capable, the learning agent eventually surpasses it given enough time. This learning process is itself autonomous and takes place while the robot is performing its job, making it comparatively inexpensive. This shows the capability of learning agents, which can also be thought of as working out a general way to perform an “expert manual tuning” process for any kind of task. Learning systems have the ability to create the entire control algorithm for the robot, and are not limited to tuning a few parameters in a script. The key step in this work allows these real-world learning systems to autonomously collect the data needed to enable the success of learning methods.

<i>This post is based on the paper “Fully Autonomous Real-World Reinforcement Learning with Applications to Mobile Manipulation”, presented at CoRL 2021. You can find more details in [our paper][4], on our [website][5] and the on the [video][6]. We provide [code][7] to reproduce our experiments. We thank Sergey Levine for his valuable feedback on this blog post.</i>

[1]:https://deepmind.com/research/case-studies/alphago-the-story-so-far
[2]:https://www.youtube.com/watch?v=XUW0cnvqbwM
[3]:https://bair.berkeley.edu/blog/2020/04/27/ingredients/
[4]:https://arxiv.org/abs/2107.13545
[5]:https://sites.google.com/view/relmm
[6]:https://youtu.be/PcYJoCe4Kr4
[7]:https://github.com/charlesjsun/ReLMM
